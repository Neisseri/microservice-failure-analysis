#ifndef __GEARBOX_COLLECTION_H__
#define __GEARBOX_COLLECTION_H__

#include "all.h"
#include "utils.h"
#include "structs.h"
#include "maps.h"

/**
 * Check if the pid is in pid_map, which is generated by Gearbox agent at user space
 */
static inline bool is_filtered_pid(u32 tgid) {
    u32 *find_tgid = bpf_map_lookup_elem(&pid_map, &tgid);

    // For the pid that is not in the pid_map, we do not need to collect the data
    if (!find_tgid) { 
        return true;
    }

    // TODO: This might introduce fragmentation in traces.
    // Currently, when will the tgid be 0?
    if (tgid == 0) {
        bpf_printk("is_filtered_pid() error: tgid is 0, which is not likely to happen\n");
        return true;
    }

    return false;
}

/**
 * Propagates context at the time of recving a packet. This function checks for existing
 * context based on flow tuple, generates and transimits points and edges, and propgates the context 
 * from flow_context_map to pid_context_map. For INGRESS components, it also generates new contexts.
 */
static inline void context_propagate_at_recv(u32 tgid, struct tcp_sock *tsk, u16 sport, u16 dport) {
    // Retrieve msg_context from flow_context_map
    u32 skc_saddr = bpf_ntohl(BPF_CORE_READ(tsk, inet_conn.icsk_inet.inet_saddr));
    u32 skc_daddr = bpf_ntohl(BPF_CORE_READ(tsk, inet_conn.icsk_inet.sk.__sk_common.skc_daddr));
    struct msg_context *_context, context = {0};
    struct flow_tuple f_tuple = {
        .sport = sport,
        .dport = dport,
        .sip = skc_saddr,
        .dip = skc_daddr,
    };
    _context = bpf_map_lookup_elem(&flow_context_map, &f_tuple);

    // Additional checks for ingress components
    // bool _is_ingress = is_ingress(tgid), _find_client_tuple = false;
    bool _is_ingress = is_ingress(tgid);
    // if (_is_ingress) {
    //     u32 key = 1;
    //     struct flow_tuple *client_tuple = bpf_map_lookup_elem(&client_ip_map, &key);

    //     if (!client_tuple) {  // This could happen, as the client_ip_map may be empty
    //         _find_client_tuple = false;
    //     } else {
    //         _find_client_tuple = true;
    //     }
    // }

    // Generating context
    if (!_context) {
        // INGRESS component has receieved a new request
        if (_is_ingress ) {
            // Update the client_ip_map, which marks the start of a new request
            u32 key = 1;
            bpf_map_update_elem(&client_ip_map, &key, &f_tuple, BPF_ANY);

            // Create a new context
            context.trace_id = ((u64)HOST_IP << 32) + (bpf_ktime_get_ns() & 0xFFFFFFFF);
            context.sender_id = 0;
            context.invoke_id = 0;

            // Update the now_trace_id_map, which checks the consistency of the trace_id
            bpf_map_update_elem(&now_trace_id_map, &key, &context.trace_id, BPF_ANY);
        } else {
            // If the component is not INGRESS
            // OR
            // the component is INGRESS and find client tuple
            // This should not happen since it indicates that this is not a new message (client_ip_map is updated)
            // but there is no context in flow_context_map (which only happens when it is a new message)
            bpf_printk("context_propagate_at_recv() error: Failed to get msg_context from flow_context_map\n");
            return;
        }
    } else {
        bpf_probe_read(&context, sizeof(context), _context);

        // Additional checks for ingress components
        if (_is_ingress) {
            u32 key = 1;

            // Also, now_trace_id should be valid and consistent with the context.trace_id
            // TODO: 据说这里是因为可能会有bug，导致trace_id更新不正常，然后就一直是这个trace_id了
            u64 *now_trace_id = bpf_map_lookup_elem(&now_trace_id_map, &key);
            if (!now_trace_id) {
                bpf_printk("context_propagate_at_send() error: now_trace_id is NULL\n");
                // TODO: 为什么是delete?
                bpf_map_delete_elem(&pid_context_map, &tgid);
                return;
            } else if (*now_trace_id != context.trace_id) {
                bpf_printk("context_propagate_at_send() error: now_trace_id is not the same as the context.trace_id\n");
                // TODO: 为什么是delete?
                bpf_map_delete_elem(&pid_context_map, &tgid);
                return;
            }
        }
    }

    // Collect metrics
    struct metrics m;
    collect_metrics(tsk, &m);

    // Generate struct point at recv side and transmit it to the user space via ring buffer
    u16 invoke_id = update_invoke_state(context.trace_id, tgid, &m);
    struct point recv_p = {
        .trace_id = context.trace_id,
        .component_id = ((u64)HOST_IP << 32) + tgid,
        .invoke_id = invoke_id,
    };
    struct point_msg p_msg = {
        .type = POINT, 
        .p = recv_p, 
        .m = m,
    };
    bpf_ringbuf_output(&rb, &p_msg, sizeof(p_msg), 0);

    // Generate struct edge at recv side and transmit it to the user space via ring buffer
    struct edge recv_e = {0};
    construct_edge_at_recv(&recv_e, context, recv_p);
    struct edge_msg e_msg = {.type = EDGE, .e = recv_e};
    bpf_ringbuf_output(&rb, &e_msg, sizeof(e_msg), 0);

    // Conduct the PROPAGATION:
    // Update the context from flow_context_map to pid_context_map
    bpf_map_update_elem(&pid_context_map, &tgid, &context, BPF_ANY);
}

/**
 * Propagates context at the time of sending a packet. This function checks for existing
 * context based on TGID, generates and transimits points, and propgates the context 
 * from pid_context_map to flow_context_map. For INGRESS components, it also generates ending edges.
 */
static inline void context_propagate_at_send(u32 tgid, struct tcp_sock *tsk, u32 saddr, u32 daddr, u16 sport, u16 dport) {
    // Retrieve msg_context from pid_context_map
    struct msg_context *_context, context = {0};
    _context = bpf_map_lookup_elem(&pid_context_map, &tgid);
    if (!_context) {
        bpf_printk("context_propagate_at_send() error: retrieve NULL _context for pid %u pid_context map\n", tgid);
        return;
    }
    bpf_probe_read(&context, sizeof(context), _context);

    // Additional checks for ingress components
    // bool _is_ingress = is_ingress(tgid), _is_client_tuple = false;
    // if (_is_ingress) {
    //     u32 key = 1;
    //     struct flow_tuple *client_tuple = bpf_map_lookup_elem(&client_ip_map, &key);

    //     // This should not happen since the client_ip_map should be updated 
    //     // when ingress component parse a FIRST_RECV syscall at context_propagate_at_recv.
    //     // At this send time, the client_ip_map should not be empty
    //     if (!client_tuple) {
    //         bpf_printk("context_propgate_at_send() error: Failed to get client_tuple from client_ip_map\n");
    //         return;
    //     }

    //     // Also, now_trace_id should be valid and consistent with the context.trace_id
    //     // TODO: 据说这里是因为可能会有bug，导致trace_id更新不正常，然后就一直是这个trace_id了
    //     u64 *now_trace_id = bpf_map_lookup_elem(&now_trace_id_map, &key);
    //     if (!now_trace_id) {
    //         bpf_printk("context_propagate_at_send() error: now_trace_id is NULL\n");
    //         return;
    //     } else if (*now_trace_id != context.trace_id) {
    //         bpf_printk("context_propagate_at_send() error: now_trace_id is not the same as the context.trace_id\n");
    //         return;
    //     }
    // }

    // Collect metrics
    struct metrics m;
    collect_metrics(tsk, &m);

    // Generate struct point at send side and transmit it to the user space via ring buffer
    u16 invoke_id = update_invoke_state(context.trace_id, tgid, &m);
    struct point send_p = {
        .trace_id = context.trace_id,
        .component_id = ((u64)HOST_IP << 32) + tgid,
        .invoke_id = invoke_id,
    };
    struct point_msg p_msg = {
        .type = POINT, 
        .p = send_p, 
        .m = m,
    };
    bpf_ringbuf_output(&rb, &p_msg, sizeof(p_msg), 0);

    // If the component is not INGRESS 
    // OR 
    // the component is INGRESS and the client_tuple is the not same as the current flow tuple
    if (1) {
        // Conduct the PROPAGATION:
        // Update flow_context_map, which will be used by tcp_int_add_tcpopt() to inject TCP options
        struct flow_tuple f_tuple = {
            .sip = saddr,
            .dip = daddr,
            .sport = sport,
            .dport = dport,
        };

        context.sender_id = ((u64)HOST_IP << 32) + tgid;
        context.invoke_id = invoke_id;
        bpf_map_update_elem(&flow_context_map, &f_tuple, &context, BPF_ANY);
    } 

    bpf_map_delete_elem(&pid_context_map, &tgid);
}

/**
 * Function to process the entry of the `recv` types of system calls.
 * This function extracts socket file descriptor from ctx->args[0].
 * Then, it gets the struct tcp_sock from the socket file descriptor,
 * wraps the struct tcp_sock with the data_args struct, and stores it in the read_args_map.
 */
static inline void process_enter_recv(struct trace_event_raw_sys_enter *ctx, enum syscall_name type) {
    // Filter the system call if the pid is not in the pid_map
    u32 tgid = bpf_get_current_pid_tgid() >> 32;
    if (is_filtered_pid(tgid)) {
        return;
    }

    // Retrieve the socket file descriptor from the system call arguments
    u32 sockfd = ctx->args[0];

    // Retrieve the struct tcp_sock from the socket file descriptor
    struct task_struct *curr_task = (struct task_struct *)bpf_get_current_task();
    struct tcp_sock *tsk = get_tsk_from_fd(sockfd, curr_task);
    if (tsk == NULL) {
        bpf_printk("process_enter_recv() error: Failed to get struct tcp_sock, tsk is NULL\n");
        return;
    }

    // Wrap the struct tcp_sock with the data_args struct, 
    // store it with tgid as the key in the read_args_map
    struct data_args read_args = {};
    read_args.tsk = tsk;
    bpf_map_update_elem(&read_args_map, &tgid, &read_args, BPF_ANY);
}

static inline void process_exit_recv(struct trace_event_raw_sys_exit *ctx, enum syscall_name type) {
    // Filter the system call if the pid is not in the pid_map
    u32 tgid = bpf_get_current_pid_tgid() >> 32;
    if (is_filtered_pid(tgid)) {
        return;
    }

    // Additional checks for buffer length
    // TODO: 这个强制转换怎么怪怪的
    u32 buf_len = (__u64)ctx->ret;
    if (0 == buf_len || 4294967280 <= buf_len) {
        bpf_printk("process_exit_recv() error: buf_len is %u, which is not in the range (0, 4294967280)\n", buf_len);
        return;
    }

    // Retrieve the struct tcp_sock from the read_args_map
    struct data_args *read_args = bpf_map_lookup_elem(&read_args_map, &tgid);
    if (!read_args) {
        // TODO: 这可能是正常情况，因为最后删掉了read_args_map
        bpf_printk("process_exit_recv() error: Failed to get struct tcp_sock from read_args_map\n");
        return;
    }
    struct tcp_sock *tsk = read_args->tsk;
    if (tsk == NULL) {
        bpf_printk("process_exit_recv() error: Failed to get struct tcp_sock from read_args_map, tsk is NULL\n");
        return;
    }

    // Filter the system call if any of the dest port, source port, and copied_seq is 0
    // TODO: This might introduce fragmentation in traces. (This is mainly inspired by DeepFlow)
    // When will these values be zero?
    u32 sport = bpf_ntohs(BPF_CORE_READ(tsk, inet_conn.icsk_inet.inet_sport));
    u32 dport = bpf_ntohs(BPF_CORE_READ(tsk, inet_conn.icsk_inet.sk.__sk_common.skc_dport));
    u32 copied_seq = BPF_CORE_READ(tsk, copied_seq);
    if (sport == 0 || dport == 0 || copied_seq == 0) {
        bpf_printk("process_exit_recv() error: sport %u, dport %u, or copied_seq %u is 0\n", sport, dport, copied_seq);
        return;
    }

    enum IO_state io_s = infer_IO_state(tgid, sport, RECV_TYPE);
    if (io_s != FIRST_RECV && io_s != NOT_FIRST_RECV) {
        bpf_printk("process_exit_recv() error: io_s is %u, which is not FIRST_RECV or NOT_FIRST_RECV\n", io_s);
        return;
    }

    // u32 skc_saddr = bpf_ntohl(BPF_CORE_READ(tsk, inet_conn.icsk_inet.inet_saddr));
    // u32 skc_daddr = bpf_ntohl(BPF_CORE_READ(tsk, inet_conn.icsk_inet.sk.__sk_common.skc_daddr));
    if (io_s == FIRST_RECV) {
        context_propagate_at_recv(tgid, tsk, sport, dport);
    }

    bpf_map_delete_elem(&read_args_map, &tgid);
}

/**
 * Function to process the entry of the `send` types of system calls.
 * This function extracts socket file descriptor from ctx->args[0].
 * Then, it gets the struct tcp_sock from the socket file descriptor,
 * infer the I/O state, adn attach context to tuple if I/O state is FIRST_SEND.
 */
static inline void process_enter_send(struct trace_event_raw_sys_enter *ctx, enum syscall_name type) {
    // Filter the system call if the pid is not in the pid_map
    u32 tgid = bpf_get_current_pid_tgid() >> 32;
    if (is_filtered_pid(tgid)) {
        return;
    }

    // Retrieve the socket file descriptor from the system call arguments
    u32 sockfd = ctx->args[0];

    // Retrieve the struct tcp_sock from the socket file descriptor
    struct task_struct *curr_task = (struct task_struct *)bpf_get_current_task();
    struct tcp_sock *tsk = get_tsk_from_fd(sockfd, curr_task);
    if (tsk == NULL) {
        bpf_printk("process_enter_send() error: Failed to get struct tcp_sock, tsk is NULL\n");
        return;
    }

    // Filter the system call if any of the dest port, source port, and write_seq is 0
    // TODO: This might introduce fragmentation in traces. (This is mainly inspired by DeepFlow)
    // When will these values be zero?
    u16 sport = bpf_ntohs(BPF_CORE_READ(tsk, inet_conn.icsk_inet.inet_sport));
    u16 dport = bpf_ntohs(BPF_CORE_READ(tsk, inet_conn.icsk_inet.sk.__sk_common.skc_dport));
    u32 write_seq = BPF_CORE_READ(tsk, write_seq);
    if (sport == 0 || dport == 0 || write_seq == 0) {
        bpf_printk("process_enter_send() error: sport %u, dport %u, or write_seq %u is 0\n", sport, dport, write_seq);
        return;
    }

    enum IO_state io_s = infer_IO_state(tgid, sport, SEND_TYPE);
    if (io_s != FIRST_SEND && io_s != NOT_FIRST_SEND) {
        bpf_printk("process_enter_send() error: io_s is %u, which is not FIRST_SEND or NOT_FIRST_SEND\n", io_s);
        return;
    }

    u32 skc_saddr = bpf_ntohl(BPF_CORE_READ(tsk, inet_conn.icsk_inet.inet_saddr));
    u32 skc_daddr = bpf_ntohl(BPF_CORE_READ(tsk, inet_conn.icsk_inet.sk.__sk_common.skc_daddr));
    if (io_s == FIRST_SEND) {
        context_propagate_at_send(tgid, tsk, skc_saddr, skc_daddr, sport, dport);
    }
}

#endif // __GEARBOX_COLLECTION_H__